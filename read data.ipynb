{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "import pandas as pd\n",
    "import json\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext\n",
    "from py4j.java_gateway import java_import\n",
    "from pyspark.mllib.common import _to_java_object_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import vnTokenizer from Java\n",
    "java_import(sc._gateway.jvm, \"vn.vitk.tok.Tokenizer\")\n",
    "Tokenizer = sc._jvm.vn.vitk.tok.Tokenizer\n",
    "dataFolder = os.getcwd() + '/dat/tok'\n",
    "token = Tokenizer(sc._jsc, dataFolder + \"/lexicon.xml\", dataFolder + \"/regexp.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# try multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def tok(read_path, write_path, filename):\n",
    "#     try:\n",
    "#         with open(read_path+ '/'+ filename,'r') as fd:\n",
    "#             json_data = json.load(fd)\n",
    "\n",
    "#         title = json_data['title']\n",
    "#         title = title.replace('\\t', ' ')\n",
    "#         title = title.replace('\\n', ' ')\n",
    "\n",
    "#         content = json_data['content']\n",
    "#         content = content.replace('\\t', ' ')\n",
    "#         content = content.replace('\\n', ' ')\n",
    "\n",
    "#         title = ' '.join(title.split())\n",
    "#         content = ' '.join(content.split())\n",
    "\n",
    "#         if(content==title==\"\"):\n",
    "#             return\n",
    "\n",
    "#         # write data\n",
    "#         with open(write_path+ '/' + filename,'w') as fd:\n",
    "#             title_tokened = token.tokenizeOneLine(title)\n",
    "#             content_tokened = token.tokenizeOneLine(content)\n",
    "#             fd.write(title_tokened)\n",
    "#             fd.write(' ')\n",
    "#             fd.write(content_tokened)\n",
    "#     except:\n",
    "#         bug_path = os.getcwd() + 'Data/log/except.txt' \n",
    "#         with open(bug_path,'a') as fd:\n",
    "#             fd.write(read_path+ '/'+ filename)\n",
    "#             fd.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# topics = ['GiaoDuc', 'PhapLuat', 'TheGioi', 'TheThao', 'ThoiSu']\n",
    "# topics = ['GiaoDuc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-10-23-13-39-53-732.txt\n",
      "2017-10-23-13-36-54-6.txt\n",
      "2017-10-23-13-37-02-430.txt\n",
      "2017-10-23-13-40-18-318.txt\n",
      "2017-10-23-13-40-46-930.txt\n",
      "2017-10-23-13-40-47-371.txt\n",
      "2017-10-23-13-41-54-426.txt\n",
      "2017-10-23-13-44-06-670.txt\n",
      "2017-10-23-13-43-36-715.txt\n",
      "2017-10-23-13-42-16-575.txt\n",
      "2017-10-23-13-43-38-162.txt\n",
      "2017-10-23-13-42-26-145.txt\n",
      "2017-10-23-13-42-28-207.txt\n",
      "2017-10-23-13-42-51-795.txt\n",
      "2017-10-23-13-48-08-903.txt\n",
      "2017-10-23-13-49-48-649.txt\n",
      "2017-10-23-13-46-10-883.txt\n",
      "2017-10-23-13-50-29-943.txt\n",
      "2017-10-23-13-51-36-833.txt\n",
      "2017-10-23-13-50-31-283.txt\n",
      "2017-10-23-13-50-23-770.txt\n",
      "2017-10-23-13-53-19-683.txt\n",
      "2017-10-23-14-07-45-390.txt\n",
      "2017-10-23-13-53-04-617.txt\n",
      "2360.536851167679\n"
     ]
    }
   ],
   "source": [
    "# start = time.time()\n",
    "\n",
    "# for topic in topics:\n",
    "\n",
    "#     read_path = os.getcwd() + '/Data/Raw/' + topic\n",
    "#     write_path = os.getcwd() + '/Data/Tokenized/' + topic\n",
    "#     if(topic=='GiaoDuc'):\n",
    "#         filenames = os.listdir(read_path)[1000:]\n",
    "#     else:\n",
    "#         filenames = os.listdir(read_path)\n",
    "#     with Pool(4) as p:\n",
    "#         p.map(partial(tok, read_path, write_path), filenames)\n",
    "\n",
    "# end = time.time()\n",
    "# print(end - start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# without mp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topics = ['GiaoDuc', 'PhapLuat', 'TheGioi', 'TheThao', 'ThoiSu']\n",
    "# topics = ['GiaoDuc']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99\n",
      "GiaoDuc Start at 99\n",
      "1000 1.3345255094551132\n",
      "2631.800597667694\n",
      "2000 2.6690510189102263\n",
      "5290.330135583878\n",
      "3000 4.00357652836534\n",
      "7928.733065128326\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for topic in topics:\n",
    "    count = 0\n",
    "\n",
    "    read_path = os.getcwd() + '/Data/Raw/' + topic\n",
    "    write_path = os.getcwd() + '/Data/Tokenized/' + topic\n",
    "    \n",
    "    tmp_arr = []\n",
    "    \n",
    "    # resume\n",
    "    list_tokened = os.listdir(write_path)\n",
    "    list_raw = os.listdir(read_path)\n",
    "    \n",
    "    if(len(list_tokened)):\n",
    "        last_file = list_tokened[-1]\n",
    "        ind = list_tokened.index(last_file)\n",
    "    else:\n",
    "        ind = 0\n",
    "        \n",
    "    print(topic + ' Start at ' + str(ind))\n",
    "    \n",
    "    for filename in list_raw[ind:]:\n",
    "        \n",
    "#         try:\n",
    "        \n",
    "        # read data\n",
    "        with open(read_path+ '/'+ filename,'r') as fd:\n",
    "            json_data = json.load(fd)\n",
    "\n",
    "        title = json_data['title']\n",
    "        title = title.replace('\\t', ' ')\n",
    "        title = title.replace('\\n', ' ')\n",
    "\n",
    "        content = json_data['content']\n",
    "        content = content.replace('\\t', ' ')\n",
    "        content = content.replace('\\n', ' ')\n",
    "\n",
    "        title = ' '.join(title.split())\n",
    "        content = ' '.join(content.split())\n",
    "\n",
    "        if(content==title==\"\"):\n",
    "            bug_path = os.getcwd() + '/Data/log/null.txt' \n",
    "            with open(bug_path,'a') as fd:\n",
    "                fd.write(read_path+ '/'+ filename)\n",
    "                fd.write('\\n')\n",
    "            continue\n",
    "            \n",
    "        # tokenize\n",
    "        title_tokened = token.tokenizeOneLine(title)\n",
    "        content_tokened = token.tokenizeOneLine(content)\n",
    "        title_tokened += content_tokened\n",
    "        \n",
    "        tmp_arr.append((filename, title_tokened))\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if(count%100==0):\n",
    "            # write data\n",
    "            for fn, data in tmp_arr:\n",
    "                with open(write_path+ '/' + fn,'w') as fd:\n",
    "                    fd.write(data)\n",
    "            tmp_arr = []\n",
    "                \n",
    "        if(count%1000==0):\n",
    "            print(count, count*100/len(list_raw))\n",
    "            end = time.time()\n",
    "            print(end - start)\n",
    "#         except:\n",
    "#             bug_path = os.getcwd() + '/Data/log/except.txt' \n",
    "#             with open(bug_path,'a') as fd:\n",
    "#                 fd.write(read_path+ '/'+ filename)\n",
    "#                 fd.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000 1.3345255094551132\n",
      "2946.1612083911896\n",
      "2000 2.6690510189102263\n",
      "5957.715993642807\n",
      "3000 4.00357652836534\n",
      "9872.391820907593\n",
      "4000 5.338102037820453\n",
      "13365.824478387833\n",
      "5000 6.6726275472755665\n",
      "16974.055755615234\n",
      "6000 8.00715305673068\n",
      "20596.21934556961\n",
      "7000 9.341678566185793\n",
      "24254.913489103317\n",
      "8000 10.676204075640905\n",
      "27952.09667277336\n",
      "9000 12.01072958509602\n",
      "31655.367478370667\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for topic in topics:\n",
    "    count = 0\n",
    "\n",
    "    read_path = os.getcwd() + '/Data/Raw/' + topic\n",
    "    write_path = os.getcwd() + '/Data/Tokenized/' + topic\n",
    "\n",
    "    for filename in os.listdir(read_path):\n",
    "        try:\n",
    "            # read data\n",
    "            with open(read_path+ '/'+ filename,'r') as fd:\n",
    "                json_data = json.load(fd)\n",
    "\n",
    "            title = json_data['title']\n",
    "            title = title.replace('\\t', ' ')\n",
    "            title = title.replace('\\n', ' ')\n",
    "\n",
    "            content = json_data['content']\n",
    "            content = content.replace('\\t', ' ')\n",
    "            content = content.replace('\\n', ' ')\n",
    "\n",
    "            title = ' '.join(title.split())\n",
    "            content = ' '.join(content.split())\n",
    "\n",
    "            if(content==title==\"\"):\n",
    "                bug_path = os.getcwd() + '/Data/log/null.txt' \n",
    "                with open(bug_path,'a') as fd:\n",
    "                    fd.write(read_path+ '/'+ filename)\n",
    "                    fd.write('\\n')\n",
    "                continue\n",
    "\n",
    "            # write data\n",
    "            with open(write_path+ '/' + filename,'a') as fd:\n",
    "                title_tokened = token.tokenizeOneLine(title)\n",
    "                content_tokened = token.tokenizeOneLine(content)\n",
    "                fd.write(title_tokened)\n",
    "                fd.write(' ')\n",
    "                fd.write(content_tokened)\n",
    "\n",
    "            count += 1\n",
    "            if(count%1000==0):\n",
    "                print(count, count*100/len(os.listdir(read_path)))\n",
    "                end = time.time()\n",
    "                print(end - start)\n",
    "        except:\n",
    "            bug_path = os.getcwd() + '/Data/log/except.txt' \n",
    "            with open(bug_path,'a') as fd:\n",
    "                fd.write(read_path+ '/'+ filename)\n",
    "                fd.write('\\n')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
